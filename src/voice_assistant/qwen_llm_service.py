"""Qwen LLM Service - åŸºäº Pipecat å®˜æ–¹æ¡†æ¶çš„ Qwen-Plus é›†æˆ

å®Œå…¨å…¼å®¹ Pipecat çš„ LLMService æ¥å£ï¼Œäº«å—å®˜æ–¹ Context Aggregator çš„ä¾¿åˆ©ã€‚
"""
import os
from typing import Any

from pipecat.services.openai.llm import OpenAILLMService
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext
from pipecat.adapters.schemas.function_schema import FunctionSchema
from pipecat.adapters.schemas.tools_schema import ToolsSchema

from .config import DASHSCOPE_API_KEY, DASHSCOPE_API_URL


def mcp_tools_to_function_schemas(mcp_tools: list[dict]) -> list[FunctionSchema]:
    """
    å°† MCP å·¥å…·åˆ—è¡¨è½¬æ¢ä¸º Pipecat FunctionSchema åˆ—è¡¨

    MCP å·¥å…·æ ¼å¼ï¼ˆæ¥è‡ª MCP Protocolï¼‰ï¼š
    {
        "name": "browser_navigate",
        "description": "Navigate to a URL",
        "input_schema": {
            "type": "object",
            "properties": {
                "url": {"type": "string", "description": "URL to navigate to"}
            },
            "required": ["url"]
        }
    }

    Pipecat FunctionSchema æ ¼å¼ï¼š
    FunctionSchema(
        name="browser_navigate",
        description="Navigate to a URL",
        properties={"url": {"type": "string", "description": "URL to navigate to"}},
        required=["url"]
    )

    Args:
        mcp_tools: MCP å·¥å…·åˆ—è¡¨

    Returns:
        Pipecat FunctionSchema åˆ—è¡¨
    """
    schemas = []
    for tool in mcp_tools:
        name = tool.get("name", "")
        description = tool.get("description", "")
        input_schema = tool.get("input_schema", {})

        properties = input_schema.get("properties", {})
        required = input_schema.get("required", [])

        # åˆ›å»º FunctionSchema
        schema = FunctionSchema(
            name=name,
            description=description,
            properties=properties,
            required=required
        )
        schemas.append(schema)

    return schemas


def create_tools_schema_from_mcp(mcp_tools: list[dict]) -> ToolsSchema:
    """
    ä» MCP å·¥å…·åˆ›å»º Pipecat ToolsSchema

    Args:
        mcp_tools: MCP å·¥å…·åˆ—è¡¨

    Returns:
        ToolsSchema å¯¹è±¡
    """
    function_schemas = mcp_tools_to_function_schemas(mcp_tools)
    return ToolsSchema(standard_tools=function_schemas)


def mcp_tools_to_openai_format(mcp_tools: list[dict]) -> list[dict]:
    """
    å°† MCP å·¥å…·åˆ—è¡¨è½¬æ¢ä¸º OpenAI API æ ¼å¼ï¼ˆç”¨äº LLMContextï¼‰

    OpenAI API æ ¼å¼ï¼š
    [
        {
            "type": "function",
            "function": {
                "name": "browser_navigate",
                "description": "Navigate to a URL",
                "parameters": {
                    "type": "object",
                    "properties": {...},
                    "required": [...]
                }
            }
        }
    ]

    Args:
        mcp_tools: MCP å·¥å…·åˆ—è¡¨

    Returns:
        OpenAI API æ ¼å¼çš„å·¥å…·åˆ—è¡¨
    """
    openai_tools = []
    for tool in mcp_tools:
        name = tool.get("name", "")
        description = tool.get("description", "")
        input_schema = tool.get("input_schema", {})

        openai_tool = {
            "type": "function",
            "function": {
                "name": name,
                "description": description,
                "parameters": input_schema  # MCP input_schema å·²ç»æ˜¯ JSON Schema æ ¼å¼
            }
        }
        openai_tools.append(openai_tool)

    return openai_tools


async def register_mcp_functions(llm_service: OpenAILLMService, mcp_manager):
    """
    å°† MCP å·¥å…·æ³¨å†Œä¸º LLM å‡½æ•°å¤„ç†å™¨

    Args:
        llm_service: QwenLLMService å®ä¾‹
        mcp_manager: MCPManager å®ä¾‹
    """
    from pipecat.services.llm_service import FunctionCallParams

    # åˆ›å»ºé€šç”¨ MCP å‡½æ•°å¤„ç†å™¨
    async def mcp_function_handler(params: FunctionCallParams):
        """ç»Ÿä¸€çš„ MCP å‡½æ•°è°ƒç”¨å¤„ç†å™¨"""
        function_name = params.function_name
        arguments = params.arguments

        print(f"ğŸ”§ è°ƒç”¨ MCP å·¥å…·: {function_name}")
        print(f"   å‚æ•°: {arguments}")

        try:
            # è°ƒç”¨ MCP å·¥å…·ï¼ˆå¼‚æ­¥ï¼‰
            result = await mcp_manager.call_tool_async(function_name, arguments)

            if result.success:
                print(f"âœ“ å·¥å…·æ‰§è¡ŒæˆåŠŸ")

                # ç®€åŒ–è¾“å‡ºï¼šç§»é™¤å†—é•¿çš„æµè§ˆå™¨ console æ—¥å¿—
                content = str(result.content) if result.content else "æ“ä½œæˆåŠŸå®Œæˆ"

                # è¿‡æ»¤æ‰æµè§ˆå™¨ console messagesï¼ˆåªä¿ç•™å…³é”®ä¿¡æ¯ï¼‰
                if "### New console messages" in content:
                    # åªä¿ç•™ç¬¬ä¸€éƒ¨åˆ†ï¼ˆRan Playwright code + Page stateï¼‰
                    parts = content.split("### New console messages")
                    content = parts[0].strip()
                    # æ·»åŠ ç®€çŸ­è¯´æ˜
                    content += "\n\n[æµè§ˆå™¨æ“ä½œæˆåŠŸ]"

                # é™åˆ¶è¾“å‡ºé•¿åº¦ï¼ˆæœ€å¤š 500 å­—ç¬¦ï¼‰
                if len(content) > 500:
                    content = content[:500] + "..."

                print(f"   ç»“æœ: {content[:100]}...")
                await params.result_callback(content)
            else:
                print(f"âœ— å·¥å…·æ‰§è¡Œå¤±è´¥: {result.error}")
                error_msg = f"é”™è¯¯: {result.error}"
                await params.result_callback(error_msg)

        except Exception as e:
            print(f"âŒ MCP è°ƒç”¨å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            error_msg = f"å¼‚å¸¸: {str(e)}"
            await params.result_callback(error_msg)

    # æ³¨å†Œæ‰€æœ‰ MCP å·¥å…·ï¼ˆä½¿ç”¨ç»Ÿä¸€å¤„ç†å™¨ï¼‰
    # ä½¿ç”¨ None ä½œä¸º function_name è¡¨ç¤ºæ•è·æ‰€æœ‰å‡½æ•°è°ƒç”¨
    llm_service.register_function(None, mcp_function_handler)

    print(f"âœ“ å·²æ³¨å†Œ MCP å‡½æ•°å¤„ç†å™¨ï¼ˆæ•è·æ‰€æœ‰å‡½æ•°è°ƒç”¨ï¼‰")


class QwenLLMService(OpenAILLMService):
    """
    Qwen LLM Service - ç»§æ‰¿è‡ª OpenAILLMService

    é˜¿é‡Œäº‘ DashScope API å®Œå…¨å…¼å®¹ OpenAI æ ¼å¼ï¼Œå› æ­¤å¯ä»¥ç›´æ¥å¤ç”¨ OpenAILLMService çš„æ‰€æœ‰åŠŸèƒ½ï¼š
    - Function Callingï¼ˆå‡½æ•°è°ƒç”¨ï¼‰
    - Streamingï¼ˆæµå¼å“åº”ï¼‰
    - Context Managementï¼ˆä¸Šä¸‹æ–‡ç®¡ç†ï¼‰

    æŠ€æœ¯ä¼˜åŠ¿ï¼š
    - âœ… ç¬¦åˆ Pipecat å®˜æ–¹æœ€ä½³å®è·µ
    - âœ… è‡ªåŠ¨ç®¡ç†å¯¹è¯å†å²ï¼ˆLLMContextAggregatorPairï¼‰
    - âœ… æ”¯æŒ Function Callingï¼ˆMCP å·¥å…·ï¼‰
    - âœ… æ”¯æŒæµå¼å“åº”
    - âœ… ä¿æŒ Qwen ç”Ÿæ€ï¼ˆä¸­æ–‡æ•ˆæœå¥½ï¼‰
    """

    def __init__(
        self,
        *,
        api_key: str | None = None,
        base_url: str | None = None,
        model: str = "qwen-plus",
        **kwargs
    ):
        """
        åˆå§‹åŒ– Qwen LLM Service

        Args:
            api_key: DashScope API Keyï¼ˆé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
            base_url: DashScope API URLï¼ˆé»˜è®¤ä½¿ç”¨ config.py é…ç½®ï¼‰
            model: Qwen æ¨¡å‹åç§°ï¼ˆqwen-plus, qwen-max, qwen-turboï¼‰
            **kwargs: ä¼ é€’ç»™ OpenAILLMService çš„å…¶ä»–å‚æ•°
        """
        # ä½¿ç”¨ DashScope API é…ç½®
        api_key = api_key or DASHSCOPE_API_KEY
        base_url = base_url or DASHSCOPE_API_URL

        # è°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°ï¼ˆOpenAILLMServiceï¼‰
        super().__init__(
            api_key=api_key,
            base_url=base_url,
            model=model,
            **kwargs
        )

        print(f"âœ“ QwenLLMService åˆå§‹åŒ–å®Œæˆ")
        print(f"  - æ¨¡å‹: {model}")
        print(f"  - API: {base_url}")


class QwenLLMContext(OpenAILLMContext):
    """
    Qwen LLM Context - ç»§æ‰¿è‡ª OpenAILLMContext

    ç”±äº Qwen API å®Œå…¨å…¼å®¹ OpenAI æ ¼å¼ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ OpenAILLMContext
    æ— éœ€ä»»ä½•ä¿®æ”¹ã€‚

    ä½¿ç”¨ç¤ºä¾‹ï¼š
    ```python
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹"},
        {"role": "user", "content": "ä½ å¥½"}
    ]
    context = QwenLLMContext(messages, tools)
    ```
    """

    def __init__(self, messages: list[dict] | None = None, tools: Any = None):
        """
        åˆå§‹åŒ– Qwen LLM Context

        Args:
            messages: å¯¹è¯å†å²ï¼ˆOpenAI æ ¼å¼ï¼‰
            tools: Function Calling å·¥å…·ï¼ˆOpenAI æ ¼å¼ï¼‰
        """
        super().__init__(messages=messages, tools=tools)
