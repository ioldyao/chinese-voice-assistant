"""Pipecat é€‚é…å™¨ - å°è£…ç°æœ‰ç»„ä»¶ä¸º Pipecat Processors"""
import asyncio
import numpy as np
import tempfile
import ctypes
from ctypes import wintypes
from pathlib import Path
from typing import Optional
from dataclasses import dataclass
from PIL import ImageGrab

from pipecat.processors.frame_processor import FrameProcessor
from pipecat.frames.frames import (
    Frame,
    AudioRawFrame,
    TextFrame,
    TTSAudioRawFrame,
    InterruptionFrame,  # âœ… å®˜æ–¹ä¸­æ–­å¸§
    TTSStoppedFrame,    # âœ… å®˜æ–¹ TTS åœæ­¢å¸§
    TranscriptionFrame,  # âœ… ç”¨äº LLMUserContextAggregator
    UserStartedSpeakingFrame,  # âœ… ç”¨æˆ·å¼€å§‹è¯´è¯
    UserStoppedSpeakingFrame,  # âœ… è§¦å‘ LLM å¤„ç†
    EndFrame,
)


# ==================== Sherpa-ONNX KWS Processor ====================

class SherpaKWSProcessor(FrameProcessor):
    """
    Sherpa-ONNX KWS é€‚é…å™¨

    å°†ç°æœ‰çš„ Sherpa-ONNX KWS æ¨¡å‹å°è£…ä¸º Pipecat Processor
    å¤„ç†éŸ³é¢‘å¸§ï¼Œæ£€æµ‹å”¤é†’è¯ï¼Œè¾“å‡ºå®˜æ–¹ InterruptionFrame
    """

    def __init__(self, kws_model):
        super().__init__()
        self.kws_model = kws_model
        self.kws_stream = kws_model.create_stream()
        self.sample_rate = 16000
        self.is_awake = False  # ç”¨äºå¹¶è¡Œ Pipeline çš„æ¡ä»¶åˆ¤æ–­
        self.last_keyword = None  # ä¿å­˜æœ€åæ£€æµ‹åˆ°çš„å”¤é†’è¯

    async def process_frame(self, frame, direction):
        """å¤„ç†éŸ³é¢‘å¸§ï¼Œæ£€æµ‹å”¤é†’è¯"""
        await super().process_frame(frame, direction)

        if isinstance(frame, AudioRawFrame):
            # æå–éŸ³é¢‘æ•°æ®
            audio_data = np.frombuffer(frame.audio, dtype=np.int16).astype(np.float32) / 32768.0

            # å–‚å…¥ KWS æ¨¡å‹
            self.kws_stream.accept_waveform(self.sample_rate, audio_data)

            # æ£€æµ‹å…³é”®è¯
            while self.kws_model.is_ready(self.kws_stream):
                self.kws_model.decode_stream(self.kws_stream)

            result = self.kws_model.get_result(self.kws_stream)

            if result:
                print(f"ğŸ”” æ£€æµ‹åˆ°å”¤é†’è¯: {result}")
                self.is_awake = True
                self.last_keyword = result

                # âœ… å‘é€ç”¨æˆ·å¼€å§‹è¯´è¯çš„ä¿¡å·
                await self.push_frame(
                    UserStartedSpeakingFrame(),
                    direction
                )

                # âœ… å‘å‡ºå®˜æ–¹ä¸­æ–­å¸§ï¼ˆè§¦å‘ ASR å¼€å§‹å½•éŸ³ï¼‰
                await self.push_frame(
                    InterruptionFrame(),
                    direction
                )

                # é‡ç½® KWS æµ
                self.kws_stream = self.kws_model.create_stream()

            # ç»§ç»­ä¼ é€’éŸ³é¢‘å¸§ï¼ˆä¾›åç»­å¤„ç†å™¨ä½¿ç”¨ï¼‰
            await self.push_frame(frame, direction)
        else:
            # å…¶ä»–å¸§ç›´æ¥ä¼ é€’
            await self.push_frame(frame, direction)


# ==================== Sherpa-ONNX ASR Processor ====================

class SherpaASRProcessor(FrameProcessor):
    """
    Sherpa-ONNX ASR é€‚é…å™¨

    å°†ç°æœ‰çš„ Sherpa-ONNX ASR æ¨¡å‹å°è£…ä¸º Pipecat Processor
    æ£€æµ‹åˆ°å”¤é†’è¯åå¼€å§‹å½•éŸ³ï¼Œä½¿ç”¨é™éŸ³æ£€æµ‹è‡ªåŠ¨åœæ­¢ï¼Œè¾“å‡ºè¯†åˆ«æ–‡æœ¬
    """

    def __init__(self, asr_model, sample_rate=16000):
        super().__init__()
        self.asr_model = asr_model
        self.sample_rate = sample_rate

        # å½•éŸ³çŠ¶æ€
        self.recording = False
        self.buffer = []

        # é™éŸ³æ£€æµ‹å‚æ•°ï¼ˆä¸åŸæœ‰é€»è¾‘ä¸€è‡´ï¼‰
        self.silence_threshold = 0.02
        self.max_silence_frames = 20  # çº¦ 1.3 ç§’
        self.min_record_frames = 15   # æœ€å°å½•éŸ³ä¿æŠ¤

        self.silence_count = 0
        self.has_speech = False
        self.frame_count = 0

    async def process_frame(self, frame, direction):
        """å¤„ç†éŸ³é¢‘å¸§ï¼Œè¯†åˆ«è¯­éŸ³"""
        await super().process_frame(frame, direction)

        # âœ… æ£€æµ‹å®˜æ–¹ä¸­æ–­å¸§ï¼Œå¼€å§‹å½•éŸ³
        if isinstance(frame, InterruptionFrame):
            print("ğŸ“ å¼€å§‹å½•éŸ³è¯†åˆ«...")
            self.recording = True
            self.buffer = []
            self.silence_count = 0
            self.has_speech = False
            self.frame_count = 0

            # ä¼ é€’ä¸­æ–­å¸§
            await self.push_frame(frame, direction)
            return

        # å½•éŸ³è¿‡ç¨‹
        if self.recording and isinstance(frame, AudioRawFrame):
            # æå–éŸ³é¢‘æ•°æ®
            audio_data = np.frombuffer(frame.audio, dtype=np.int16).astype(np.float32) / 32768.0
            self.buffer.append(audio_data)
            self.frame_count += 1

            # è®¡ç®—éŸ³é‡
            volume = np.sqrt(np.mean(audio_data**2))

            # é™éŸ³æ£€æµ‹
            if volume >= self.silence_threshold:
                self.has_speech = True
                self.silence_count = 0
            else:
                self.silence_count += 1

            # åœæ­¢æ¡ä»¶ï¼šæœ‰è¯­éŸ³ + è¿ç»­é™éŸ³ + è¶…è¿‡æœ€å°ä¿æŠ¤å¸§
            if (self.has_speech and
                self.silence_count > self.max_silence_frames and
                self.frame_count > self.min_record_frames):

                # æ‹¼æ¥éŸ³é¢‘
                full_audio = np.concatenate(self.buffer)

                # ASR è¯†åˆ«
                text = await self._recognize_async(full_audio)

                if text:
                    print(f"âœ“ è¯†åˆ«ç»“æœ: {text}")
                    # âœ… ä½¿ç”¨ TranscriptionFrameï¼ˆLLMUserContextAggregator æœŸæœ›çš„ç±»å‹ï¼‰
                    await self.push_frame(
                        TranscriptionFrame(text=text, user_id="user", timestamp=self._get_timestamp()),
                        direction
                    )
                    # å‘é€ UserStoppedSpeakingFrame è§¦å‘ LLM å¤„ç†
                    await self.push_frame(
                        UserStoppedSpeakingFrame(),
                        direction
                    )

                # é‡ç½®çŠ¶æ€
                self.recording = False
                self.buffer = []

            # ç»§ç»­ä¼ é€’éŸ³é¢‘å¸§
            await self.push_frame(frame, direction)
        else:
            # å…¶ä»–å¸§ç›´æ¥ä¼ é€’
            await self.push_frame(frame, direction)

    async def _recognize_async(self, audio_data):
        """å¼‚æ­¥ ASR è¯†åˆ«ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰"""
        def _recognize_sync():
            # åˆ›å»º ASR æµ
            asr_stream = self.asr_model.create_stream()
            asr_stream.accept_waveform(self.sample_rate, audio_data)
            self.asr_model.decode_stream(asr_stream)
            return asr_stream.result.text.strip()

        # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼ˆé¿å…é˜»å¡äº‹ä»¶å¾ªç¯ï¼‰
        return await asyncio.to_thread(_recognize_sync)

    def _get_timestamp(self):
        """è·å–å½“å‰æ—¶é—´æˆ³ï¼ˆISO 8601 æ ¼å¼ï¼‰"""
        from datetime import datetime, timezone
        return datetime.now(timezone.utc).isoformat()


# ==================== React Agent Processor ====================

class ReactAgentProcessor(FrameProcessor):
    """
    React Agent Processor - åŸºäº MCP å®˜æ–¹æ¨èæ¨¡å¼

    ä½¿ç”¨å®Œå…¨å¼‚æ­¥çš„å®ç°ï¼š
    - ç›´æ¥è°ƒç”¨ agent.execute_command_async()
    - åœ¨ä¸»äº‹ä»¶å¾ªç¯ä¸­æ‰§è¡Œ MCP è°ƒç”¨
    - åå°ä»»åŠ¡æ‰§è¡Œï¼Œä¸é˜»å¡ Pipeline
    - å“åº”å®˜æ–¹ InterruptionFrame ä¸­æ–­ä¿¡å·
    """

    def __init__(self, react_agent):
        """
        åˆå§‹åŒ– React Agent Processor

        Args:
            react_agent: ReactAgent å®ä¾‹ï¼ˆå·²åˆå§‹åŒ– MCPï¼‰
        """
        super().__init__()
        self.agent = react_agent
        self.current_task = None
        self.cancel_flag = False

    async def process_frame(self, frame, direction):
        """å¤„ç†å¸§"""
        await super().process_frame(frame, direction)

        # âœ… å“åº”å®˜æ–¹ä¸­æ–­å¸§ï¼Œå–æ¶ˆå½“å‰ä»»åŠ¡
        if isinstance(frame, InterruptionFrame):
            if self.current_task and not self.current_task.done():
                print("â¸ï¸  æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œå–æ¶ˆå½“å‰ Agent ä»»åŠ¡")
                self.cancel_flag = True
                self.current_task.cancel()
            await self.push_frame(frame, direction)
            return

        # å¤„ç†æ–‡æœ¬å‘½ä»¤
        if isinstance(frame, TextFrame):
            print(f"ğŸ¤– React Agent å¤„ç†: {frame.text}")
            self.cancel_flag = False

            # åˆ›å»ºåå°ä»»åŠ¡ï¼ˆä¸ç­‰å¾…å®Œæˆï¼‰
            self.current_task = asyncio.create_task(
                self._execute_and_push_result(frame.text, direction)
            )

            # ç«‹å³ä¼ é€’å¸§ï¼Œä¸é˜»å¡ Pipeline
            await self.push_frame(frame, direction)
        else:
            # å…¶ä»–å¸§ç›´æ¥ä¼ é€’
            await self.push_frame(frame, direction)

    async def _execute_and_push_result(self, command: str, direction):
        """
        æ‰§è¡Œå‘½ä»¤å¹¶æ¨é€ç»“æœï¼ˆåå°ä»»åŠ¡ï¼‰

        åŸºäº MCP å®˜æ–¹æ¨èæ¨¡å¼ï¼šç›´æ¥å¼‚æ­¥è°ƒç”¨ï¼Œæ— éœ€çº¿ç¨‹
        """
        try:
            print(f"âœ¨ å¼€å§‹æ‰§è¡Œå‘½ä»¤: {command}")

            # ä½¿ç”¨å®˜æ–¹æ¨èçš„å¼‚æ­¥æ–¹å¼ç›´æ¥è°ƒç”¨
            result = await self.agent.execute_command_async(command, enable_voice=False)

            print(f"âœ… æ‰§è¡Œå®Œæˆ: success={result.get('success')}")

            # æ£€æŸ¥æ˜¯å¦è¢«å–æ¶ˆ
            if self.cancel_flag:
                print("â¸ï¸  ä»»åŠ¡å·²å–æ¶ˆï¼Œä¸æ¨é€ç»“æœ")
                return

            # æ¨é€ç»“æœæ–‡æœ¬ï¼ˆå¦‚æœæˆåŠŸï¼‰
            if result.get("success") and result.get("message"):
                result_frame = TextFrame(result["message"])
                await self.push_frame(result_frame, direction)

        except asyncio.CancelledError:
            print("â¸ï¸  ä»»åŠ¡è¢«å–æ¶ˆ")
            raise
        except Exception as e:
            print(f"âŒ æ‰§è¡Œå¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()



# ==================== Piper TTS Processor ====================

class PiperTTSProcessor(FrameProcessor):
    """
    Piper TTS é€‚é…å™¨ - æ”¯æŒæ–‡æœ¬èšåˆ

    å°†ç°æœ‰çš„ TTSManagerStreaming å°è£…ä¸º Pipecat Processor
    æ¥æ”¶æ–‡æœ¬å¸§ï¼Œç”ŸæˆéŸ³é¢‘å¸§å¹¶ç›´æ¥æ’­æ”¾
    å“åº”å®˜æ–¹ InterruptionFrameï¼Œå‘å‡º TTSStoppedFrame
    èšåˆæµå¼æ–‡æœ¬ï¼Œå®Œæ•´æ’­æ”¾
    """

    def __init__(self, tts_manager, transport=None):
        super().__init__()
        self.tts = tts_manager
        self.transport = transport  # ç”¨äºç›´æ¥æ’­æ”¾éŸ³é¢‘
        self.interrupt_flag = False  # ä¸­æ–­æ ‡å¿—
        self.is_speaking = False  # TTS æ’­æ”¾çŠ¶æ€

        # âœ… æ–‡æœ¬èšåˆç¼“å†²åŒº
        self.text_buffer = []
        self.is_buffering = False

    def interrupt(self):
        """ä¸­æ–­å½“å‰TTSæ’­æ”¾"""
        self.interrupt_flag = True

    async def process_frame(self, frame, direction):
        """å¤„ç†æ–‡æœ¬å¸§ï¼Œç”Ÿæˆ TTS éŸ³é¢‘"""
        await super().process_frame(frame, direction)

        # âœ… å“åº”å®˜æ–¹ä¸­æ–­å¸§ï¼Œè®¾ç½®ä¸­æ–­
        if isinstance(frame, InterruptionFrame):
            if self.is_speaking:
                print("â¸ï¸  æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œåœæ­¢TTSæ’­æ”¾")
                self.interrupt()
            # æ¸…ç©ºç¼“å†²åŒº
            self.text_buffer = []
            self.is_buffering = False
            # ä¼ é€’ä¸­æ–­å¸§
            await self.push_frame(frame, direction)
            return

        # âœ… æ£€æµ‹ LLM å“åº”å¼€å§‹ï¼ˆå¼€å§‹ç¼“å†²ï¼‰
        from pipecat.frames.frames import LLMFullResponseStartFrame
        if isinstance(frame, LLMFullResponseStartFrame):
            self.text_buffer = []
            self.is_buffering = True
            await self.push_frame(frame, direction)
            return

        # âœ… æ£€æµ‹ LLM å“åº”ç»“æŸï¼ˆæ’­æ”¾ç¼“å†²å†…å®¹ï¼‰
        from pipecat.frames.frames import LLMFullResponseEndFrame
        if isinstance(frame, LLMFullResponseEndFrame):
            self.is_buffering = False
            if self.text_buffer:
                # èšåˆæ‰€æœ‰æ–‡æœ¬
                full_text = "".join(self.text_buffer)
                print(f"ğŸ”Š TTS åˆæˆï¼ˆå®Œæ•´ï¼‰: {full_text[:50]}...")

                # æ’­æ”¾å®Œæ•´æ–‡æœ¬
                self.interrupt_flag = False
                self.is_speaking = True

                was_interrupted = await asyncio.to_thread(
                    self._synthesize_and_play_sync,
                    full_text
                )

                self.is_speaking = False
                if was_interrupted:
                    print("â¸ï¸  TTS å·²è¢«ä¸­æ–­")
                    await self.push_frame(TTSStoppedFrame(), direction)
                else:
                    print("âœ“ TTS æ’­æ”¾å®Œæˆ")
                    # âœ… å‘é€ turn ç»“æŸä¿¡å·
                    await self.push_frame(UserStoppedSpeakingFrame(), direction)

                self.text_buffer = []

            await self.push_frame(frame, direction)
            return

        # âœ… ç¼“å†²æ–‡æœ¬å¸§ï¼ˆæµå¼è¾“å‡ºï¼‰
        if isinstance(frame, TextFrame):
            if self.is_buffering:
                # ç¼“å†²æ¨¡å¼ï¼šæ”¶é›†æ–‡æœ¬ï¼Œä¸æ’­æ”¾
                self.text_buffer.append(frame.text)
            else:
                # éç¼“å†²æ¨¡å¼ï¼ˆå…¼å®¹éæµå¼è¾“å‡ºï¼‰ï¼šç›´æ¥æ’­æ”¾
                print(f"ğŸ”Š TTS åˆæˆ: {frame.text}")
                self.interrupt_flag = False
                self.is_speaking = True

                was_interrupted = await asyncio.to_thread(
                    self._synthesize_and_play_sync,
                    frame.text
                )

                self.is_speaking = False
                if was_interrupted:
                    print("â¸ï¸  TTS å·²è¢«ä¸­æ–­")
                    await self.push_frame(TTSStoppedFrame(), direction)
                else:
                    print("âœ“ TTS æ’­æ”¾å®Œæˆ")
                    await self.push_frame(UserStoppedSpeakingFrame(), direction)

            # ä¼ é€’åŸå§‹æ–‡æœ¬å¸§
            await self.push_frame(frame, direction)
        else:
            # å…¶ä»–å¸§ç›´æ¥ä¼ é€’
            await self.push_frame(frame, direction)

    def _synthesize_and_play_sync(self, text) -> bool:
        """
        åŒæ­¥ TTS åˆæˆå¹¶æ’­æ”¾ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰ï¼Œæ”¯æŒä¸­æ–­

        Returns:
            bool: æ˜¯å¦è¢«ä¸­æ–­
        """
        if self.tts.engine_type == "piper":
            try:
                # ä½¿ç”¨ Piper TTS ç”ŸæˆéŸ³é¢‘
                audio_generator = self.tts.piper_voice.synthesize(text)

                for chunk in audio_generator:
                    # æ£€æŸ¥ä¸­æ–­æ ‡å¿—
                    if self.interrupt_flag:
                        print("â¸ï¸  TTS æ’­æ”¾å·²ä¸­æ–­")
                        return True  # è¿”å›ä¸­æ–­æ ‡å¿—

                    # æå–éŸ³é¢‘æ•°æ®
                    audio_float = chunk.audio_float_array
                    sample_rate = chunk.sample_rate

                    # è½¬æ¢ä¸º int16
                    audio_int16 = (audio_float * 32767).astype(np.int16)

                    # ç›´æ¥æ’­æ”¾åˆ° transport
                    if self.transport and self.transport.output_stream:
                        self.transport.output_stream.write(audio_int16.tobytes())

                return False  # æ­£å¸¸å®Œæˆï¼Œæœªä¸­æ–­

            except Exception as e:
                print(f"âŒ TTS ç”Ÿæˆå¤±è´¥: {e}")
                return False


# ==================== Vision Processors (Pipecat å®˜æ–¹æ¨¡å¼) ====================

class ScreenshotProcessor(FrameProcessor):
    """
    æˆªå›¾ Processor - åˆ¤æ–­å¹¶ç”Ÿæˆ UserImageRawFrame

    é‡‡ç”¨ Pipecat å®˜æ–¹æ¨èæ¨¡å¼ï¼š
    - åˆ¤æ–­æ˜¯å¦éœ€è¦è§†è§‰ç†è§£
    - æˆªå›¾å¹¶ç”Ÿæˆ UserImageRawFrameï¼ˆå†…å­˜ä¼ é€’ï¼Œæ— æ–‡ä»¶ I/Oï¼‰
    - ä¼ é€’ç»™ QwenVisionProcessor å¤„ç†
    """

    def __init__(self):
        super().__init__()
        self.vision_keywords = [
            "çœ‹", "æŸ¥çœ‹", "è®²è§£", "æè¿°", "æ˜¾ç¤ºä»€ä¹ˆ", "æ˜¾ç¤ºçš„",
            "åˆ†æ", "è¯†åˆ«", "å†…å®¹æ˜¯", "ç”»é¢", "æˆªå›¾", "å›¾ç‰‡"
        ]
        self.operation_keywords = [
            "ç‚¹å‡»", "è¾“å…¥", "æ‰“å¼€", "å…³é—­", "å¯åŠ¨", "åˆ‡æ¢",
            "æ»šåŠ¨", "æœç´¢", "æ‰§è¡Œ", "è¿è¡Œ", "æŒ‰"
        ]

    def _needs_vision(self, text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦è§†è§‰ç†è§£"""
        # æ“ä½œå…³é”®è¯ä¼˜å…ˆï¼ˆReact æ¨¡å¼ï¼‰
        if any(kw in text for kw in self.operation_keywords):
            return False
        # è§†è§‰å…³é”®è¯
        return any(kw in text for kw in self.vision_keywords)

    async def _capture_screenshot_async(self) -> tuple[bytes, tuple[int, int]]:
        """
        å¼‚æ­¥æˆªå›¾ï¼Œè¿”å› (å›¾ç‰‡å­—èŠ‚, å°ºå¯¸)

        ä½¿ç”¨ Pipecat å®˜æ–¹æ¨¡å¼ï¼šå†…å­˜ä¼ é€’ï¼Œæ— æ–‡ä»¶ I/O
        """
        def capture():
            from PIL import ImageGrab
            import io

            # å°è¯•çª—å£æˆªå›¾
            try:
                import ctypes
                from ctypes import wintypes

                # è®¾ç½® DPI æ„ŸçŸ¥
                try:
                    ctypes.windll.shcore.SetProcessDpiAwareness(2)
                except:
                    pass

                # è·å–å‰å°çª—å£
                hwnd = ctypes.windll.user32.GetForegroundWindow()
                if hwnd:
                    rect = wintypes.RECT()
                    ctypes.windll.user32.GetWindowRect(hwnd, ctypes.byref(rect))

                    # ä¿®æ­£è¾¹æ¡†
                    padding = 8
                    bbox = (
                        rect.left + padding,
                        rect.top,
                        rect.right - padding,
                        rect.bottom - padding
                    )
                    screenshot = ImageGrab.grab(bbox=bbox)
                else:
                    screenshot = ImageGrab.grab()
            except Exception:
                # é™çº§åˆ°å…¨å±
                screenshot = ImageGrab.grab()

            # è½¬æ¢ä¸ºå­—èŠ‚ï¼ˆå†…å­˜æ“ä½œï¼‰
            img_byte_arr = io.BytesIO()
            screenshot.save(img_byte_arr, format='PNG')
            return img_byte_arr.getvalue(), screenshot.size

        return await asyncio.to_thread(capture)

    async def process_frame(self, frame, direction):
        """å¤„ç†å¸§"""
        await super().process_frame(frame, direction)

        # å“åº”ä¸­æ–­
        if isinstance(frame, InterruptionFrame):
            await self.push_frame(frame, direction)
            return

        # âœ… å¤„ç†æ–‡æœ¬å‘½ä»¤ï¼ˆæ”¯æŒ TextFrame å’Œ TranscriptionFrameï¼‰
        text_content = None
        if isinstance(frame, TextFrame):
            text_content = frame.text
        elif isinstance(frame, TranscriptionFrame):
            text_content = frame.text

        if text_content:
            if self._needs_vision(text_content):
                print(f"ğŸ” Vision æ¨¡å¼: {text_content}")

                try:
                    # å¼‚æ­¥æˆªå›¾ï¼ˆå†…å­˜æ“ä½œï¼‰
                    img_bytes, size = await self._capture_screenshot_async()

                    # åˆ›å»º UserImageRawFrameï¼ˆPipecat å®˜æ–¹ Frameï¼‰
                    from pipecat.frames.frames import UserImageRawFrame

                    vision_frame = UserImageRawFrame(
                        image=img_bytes,
                        size=size,
                        format="PNG"
                    )

                    # é™„åŠ ç”¨æˆ·é—®é¢˜ï¼ˆç”¨äº Vision APIï¼‰
                    vision_frame.user_question = text_content

                    # æ¨é€åˆ° QwenVisionProcessor
                    await self.push_frame(vision_frame, direction)
                    return

                except Exception as e:
                    print(f"âŒ æˆªå›¾å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                    # å¤±è´¥æ—¶ä¼ é€’åŸå§‹å¸§ç»™ Agent
                    await self.push_frame(frame, direction)
                    return

        # é Vision ä»»åŠ¡ï¼Œä¼ é€’ç»™ä¸‹æ¸¸
        await self.push_frame(frame, direction)


class QwenVisionProcessor(FrameProcessor):
    """
    Qwen Vision API Processor - å¤„ç† UserImageRawFrame

    é‡‡ç”¨ Pipecat å®˜æ–¹æ¨èæ¨¡å¼ï¼š
    - æ¥æ”¶ UserImageRawFrameï¼ˆå®˜æ–¹ Frame ç±»å‹ï¼‰
    - è°ƒç”¨ Qwen-VL-Max APIï¼ˆå®Œå…¨å¼‚æ­¥ï¼‰
    - è¿”å› TextFrameï¼ˆç»“æœï¼‰
    """

    def __init__(self, api_url: str, api_key: str):
        super().__init__()
        self.api_url = api_url
        self.api_key = api_key

    async def _call_vision_api(self, img_bytes: bytes, question: str) -> str:
        """è°ƒç”¨ Qwen-VL-Max APIï¼ˆå¼‚æ­¥ï¼‰"""
        import httpx
        import base64

        img_base64 = base64.b64encode(img_bytes).decode()

        try:
            async with httpx.AsyncClient(timeout=60) as client:
                response = await client.post(
                    f"{self.api_url}/chat/completions",
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": "qwen-vl-max",
                        "messages": [{
                            "role": "user",
                            "content": [
                                {"type": "text", "text": question},
                                {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{img_base64}"}}
                            ]
                        }],
                        "max_tokens": 2000,
                        "temperature": 0.7
                    }
                )

            if response.status_code == 200:
                result = response.json()
                return result["choices"][0]["message"]["content"]
            else:
                return f"APIé”™è¯¯ {response.status_code}: {response.text[:100]}"

        except Exception as e:
            return f"Vision API è°ƒç”¨å¤±è´¥: {str(e)}"

    async def process_frame(self, frame, direction):
        """å¤„ç†å¸§"""
        await super().process_frame(frame, direction)

        # å“åº”ä¸­æ–­
        if isinstance(frame, InterruptionFrame):
            await self.push_frame(frame, direction)
            return

        # å¤„ç† UserImageRawFrameï¼ˆPipecat å®˜æ–¹ Frameï¼‰
        from pipecat.frames.frames import UserImageRawFrame

        if isinstance(frame, UserImageRawFrame):
            # æå–é—®é¢˜
            question = getattr(frame, 'user_question', "å±å¹•ä¸Šæœ‰ä»€ä¹ˆå†…å®¹ï¼Ÿ")

            print(f"ğŸ“¸ è°ƒç”¨ Vision API...")

            # å¼‚æ­¥è°ƒç”¨ API
            result = await self._call_vision_api(frame.image, question)

            # è¾“å‡ºç»“æœ
            print(f"ğŸ“Š Vision ç»“æœ: {result[:100]}...")

            # æ¨é€ç»“æœï¼ˆTextFrameï¼‰
            await self.push_frame(TextFrame(result), direction)
            return

        # å…¶ä»–å¸§ç±»å‹ï¼Œç›´æ¥ä¼ é€’
        await self.push_frame(frame, direction)
