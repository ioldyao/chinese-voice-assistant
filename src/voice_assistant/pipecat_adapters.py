"""Pipecat é€‚é…å™¨ - å°è£…ç°æœ‰ç»„ä»¶ä¸º Pipecat Processors"""
import asyncio
import numpy as np
from typing import Optional
from dataclasses import dataclass

from pipecat.processors.frame_processor import FrameProcessor
from pipecat.frames.frames import (
    Frame,
    AudioRawFrame,
    TextFrame,
    TTSAudioRawFrame,
    StartInterruptionFrame,
    EndFrame,
)


# ==================== è‡ªå®šä¹‰ Frame ç±»å‹ ====================

@dataclass
class WakeWordDetectedFrame(Frame):
    """å”¤é†’è¯æ£€æµ‹å¸§"""
    keyword: str
    confidence: float = 1.0


@dataclass
class ReActStepFrame(Frame):
    """React æ¨ç†æ­¥éª¤å¸§"""
    thought: str
    action: str
    action_input: dict
    observation: str
    success: bool


# ==================== Sherpa-ONNX KWS Processor ====================

class SherpaKWSProcessor(FrameProcessor):
    """
    Sherpa-ONNX KWS é€‚é…å™¨

    å°†ç°æœ‰çš„ Sherpa-ONNX KWS æ¨¡å‹å°è£…ä¸º Pipecat Processor
    å¤„ç†éŸ³é¢‘å¸§ï¼Œæ£€æµ‹å”¤é†’è¯ï¼Œè¾“å‡º WakeWordDetectedFrame
    """

    def __init__(self, kws_model):
        super().__init__()
        self.kws_model = kws_model
        self.kws_stream = kws_model.create_stream()
        self.sample_rate = 16000
        self.is_awake = False  # ç”¨äºå¹¶è¡Œ Pipeline çš„æ¡ä»¶åˆ¤æ–­

    async def process_frame(self, frame, direction):
        """å¤„ç†éŸ³é¢‘å¸§ï¼Œæ£€æµ‹å”¤é†’è¯"""
        await super().process_frame(frame, direction)

        if isinstance(frame, AudioRawFrame):
            # æå–éŸ³é¢‘æ•°æ®
            audio_data = np.frombuffer(frame.audio, dtype=np.int16).astype(np.float32) / 32768.0

            # å–‚å…¥ KWS æ¨¡å‹
            self.kws_stream.accept_waveform(self.sample_rate, audio_data)

            # æ£€æµ‹å…³é”®è¯
            while self.kws_model.is_ready(self.kws_stream):
                self.kws_model.decode_stream(self.kws_stream)

            result = self.kws_model.get_result(self.kws_stream)

            if result:
                print(f"ğŸ”” æ£€æµ‹åˆ°å”¤é†’è¯: {result}")
                self.is_awake = True

                # å‘å‡ºå”¤é†’äº‹ä»¶
                await self.push_frame(
                    WakeWordDetectedFrame(keyword=result),
                    direction
                )

                # é‡ç½® KWS æµ
                self.kws_stream = self.kws_model.create_stream()

            # ç»§ç»­ä¼ é€’éŸ³é¢‘å¸§ï¼ˆä¾›åç»­å¤„ç†å™¨ä½¿ç”¨ï¼‰
            await self.push_frame(frame, direction)
        else:
            # å…¶ä»–å¸§ç›´æ¥ä¼ é€’
            await self.push_frame(frame, direction)


# ==================== Sherpa-ONNX ASR Processor ====================

class SherpaASRProcessor(FrameProcessor):
    """
    Sherpa-ONNX ASR é€‚é…å™¨

    å°†ç°æœ‰çš„ Sherpa-ONNX ASR æ¨¡å‹å°è£…ä¸º Pipecat Processor
    æ£€æµ‹åˆ°å”¤é†’è¯åå¼€å§‹å½•éŸ³ï¼Œä½¿ç”¨é™éŸ³æ£€æµ‹è‡ªåŠ¨åœæ­¢ï¼Œè¾“å‡ºè¯†åˆ«æ–‡æœ¬
    """

    def __init__(self, asr_model, sample_rate=16000):
        super().__init__()
        self.asr_model = asr_model
        self.sample_rate = sample_rate

        # å½•éŸ³çŠ¶æ€
        self.recording = False
        self.buffer = []

        # é™éŸ³æ£€æµ‹å‚æ•°ï¼ˆä¸åŸæœ‰é€»è¾‘ä¸€è‡´ï¼‰
        self.silence_threshold = 0.02
        self.max_silence_frames = 20  # çº¦ 1.3 ç§’
        self.min_record_frames = 15   # æœ€å°å½•éŸ³ä¿æŠ¤

        self.silence_count = 0
        self.has_speech = False
        self.frame_count = 0

    async def process_frame(self, frame, direction):
        """å¤„ç†éŸ³é¢‘å¸§ï¼Œè¯†åˆ«è¯­éŸ³"""
        await super().process_frame(frame, direction)

        # æ£€æµ‹å”¤é†’è¯ï¼Œå¼€å§‹å½•éŸ³
        if isinstance(frame, WakeWordDetectedFrame):
            print("ğŸ“ å¼€å§‹å½•éŸ³è¯†åˆ«...")
            self.recording = True
            self.buffer = []
            self.silence_count = 0
            self.has_speech = False
            self.frame_count = 0

            # ä¼ é€’å”¤é†’å¸§
            await self.push_frame(frame, direction)
            return

        # å½•éŸ³è¿‡ç¨‹
        if self.recording and isinstance(frame, AudioRawFrame):
            # æå–éŸ³é¢‘æ•°æ®
            audio_data = np.frombuffer(frame.audio, dtype=np.int16).astype(np.float32) / 32768.0
            self.buffer.append(audio_data)
            self.frame_count += 1

            # è®¡ç®—éŸ³é‡
            volume = np.sqrt(np.mean(audio_data**2))

            # é™éŸ³æ£€æµ‹
            if volume >= self.silence_threshold:
                self.has_speech = True
                self.silence_count = 0
            else:
                self.silence_count += 1

            # åœæ­¢æ¡ä»¶ï¼šæœ‰è¯­éŸ³ + è¿ç»­é™éŸ³ + è¶…è¿‡æœ€å°ä¿æŠ¤å¸§
            if (self.has_speech and
                self.silence_count > self.max_silence_frames and
                self.frame_count > self.min_record_frames):

                # æ‹¼æ¥éŸ³é¢‘
                full_audio = np.concatenate(self.buffer)

                # ASR è¯†åˆ«
                text = await self._recognize_async(full_audio)

                if text:
                    print(f"âœ“ è¯†åˆ«ç»“æœ: {text}")
                    await self.push_frame(
                        TextFrame(text=text),
                        direction
                    )

                # é‡ç½®çŠ¶æ€
                self.recording = False
                self.buffer = []

            # ç»§ç»­ä¼ é€’éŸ³é¢‘å¸§
            await self.push_frame(frame, direction)
        else:
            # å…¶ä»–å¸§ç›´æ¥ä¼ é€’
            await self.push_frame(frame, direction)

    async def _recognize_async(self, audio_data):
        """å¼‚æ­¥ ASR è¯†åˆ«ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰"""
        def _recognize_sync():
            # åˆ›å»º ASR æµ
            asr_stream = self.asr_model.create_stream()
            asr_stream.accept_waveform(self.sample_rate, audio_data)
            self.asr_model.decode_stream(asr_stream)
            return asr_stream.result.text.strip()

        # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼ˆé¿å…é˜»å¡äº‹ä»¶å¾ªç¯ï¼‰
        return await asyncio.to_thread(_recognize_sync)


# ==================== React Agent Processor ====================

class ReactAgentProcessor(FrameProcessor):
    """
    React Agent é€‚é…å™¨

    å°†ç°æœ‰çš„ ReactAgent å°è£…ä¸º Pipecat Processor
    æ¥æ”¶æ–‡æœ¬å¸§ï¼Œè°ƒç”¨ execute_commandï¼Œè¾“å‡ºå“åº”æ–‡æœ¬
    """

    def __init__(self, react_agent):
        super().__init__()
        self.agent = react_agent

    async def process_frame(self, frame, direction):
        """å¤„ç†æ–‡æœ¬å¸§ï¼Œæ‰§è¡Œ React Agent"""
        await super().process_frame(frame, direction)

        if isinstance(frame, TextFrame):
            print(f"ğŸ¤– React Agent å¤„ç†: {frame.text}")

            # åœ¨çº¿ç¨‹æ± ä¸­è°ƒç”¨åŒæ­¥çš„ execute_command
            result = await asyncio.to_thread(
                self.agent.execute_command,
                frame.text,
                enable_voice=False  # TTS ç”± Pipecat ç®¡ç†
            )

            if result.get("success"):
                response = result.get("message", "")
                if response:
                    print(f"ğŸ’¬ å“åº”: {response}")
                    await self.push_frame(
                        TextFrame(text=response),
                        direction
                    )
            else:
                error_msg = result.get("message", "æ‰§è¡Œå¤±è´¥")
                print(f"âŒ é”™è¯¯: {error_msg}")
                await self.push_frame(
                    TextFrame(text=f"æŠ±æ­‰ï¼Œ{error_msg}"),
                    direction
                )

            # ä¼ é€’åŸå§‹å¸§
            await self.push_frame(frame, direction)
        else:
            # å…¶ä»–å¸§ç›´æ¥ä¼ é€’
            await self.push_frame(frame, direction)


# ==================== Piper TTS Processor ====================

class PiperTTSProcessor(FrameProcessor):
    """
    Piper TTS é€‚é…å™¨

    å°†ç°æœ‰çš„ TTSManagerStreaming å°è£…ä¸º Pipecat Processor
    æ¥æ”¶æ–‡æœ¬å¸§ï¼Œç”ŸæˆéŸ³é¢‘å¸§
    """

    def __init__(self, tts_manager):
        super().__init__()
        self.tts = tts_manager

    async def process_frame(self, frame, direction):
        """å¤„ç†æ–‡æœ¬å¸§ï¼Œç”Ÿæˆ TTS éŸ³é¢‘"""
        await super().process_frame(frame, direction)

        if isinstance(frame, TextFrame):
            print(f"ğŸ”Š TTS åˆæˆ: {frame.text}")

            # åœ¨çº¿ç¨‹æ± ä¸­ç”ŸæˆéŸ³é¢‘
            audio_chunks = await asyncio.to_thread(
                self._synthesize_sync,
                frame.text
            )

            # æ¨é€éŸ³é¢‘å¸§
            for chunk in audio_chunks:
                await self.push_frame(chunk, direction)

            # ä¼ é€’åŸå§‹æ–‡æœ¬å¸§
            await self.push_frame(frame, direction)
        else:
            # å…¶ä»–å¸§ç›´æ¥ä¼ é€’
            await self.push_frame(frame, direction)

    def _synthesize_sync(self, text):
        """åŒæ­¥ TTS åˆæˆï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰"""
        chunks = []

        if self.tts.engine_type == "piper":
            try:
                # ä½¿ç”¨ Piper TTS ç”ŸæˆéŸ³é¢‘
                audio_generator = self.tts.piper_voice.synthesize(text)

                for chunk in audio_generator:
                    # æå–éŸ³é¢‘æ•°æ®
                    audio_float = chunk.audio_float_array
                    sample_rate = chunk.sample_rate

                    # è½¬æ¢ä¸º int16
                    audio_int16 = (audio_float * 32767).astype(np.int16)

                    # åˆ›å»º TTS éŸ³é¢‘å¸§
                    audio_frame = TTSAudioRawFrame(
                        audio=audio_int16.tobytes(),
                        sample_rate=sample_rate,
                        num_channels=1
                    )
                    chunks.append(audio_frame)

            except Exception as e:
                print(f"âŒ TTS ç”Ÿæˆå¤±è´¥: {e}")

        return chunks
