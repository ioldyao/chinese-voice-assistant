"""Pipecat é€‚é…å™¨ - å°è£…ç°æœ‰ç»„ä»¶ä¸º Pipecat Processors"""
import asyncio
import numpy as np
import tempfile
import ctypes
from ctypes import wintypes
from pathlib import Path
from typing import Optional
from dataclasses import dataclass
from PIL import ImageGrab

from pipecat.processors.frame_processor import FrameProcessor, FrameDirection
from pipecat.frames.frames import (
    Frame,
    AudioRawFrame,
    TextFrame,
    TTSAudioRawFrame,
    OutputAudioRawFrame,  # âœ… TTS éŸ³é¢‘è¾“å‡ºå¸§
    InterruptionFrame,  # âœ… å®˜æ–¹ä¸­æ–­å¸§
    TTSStoppedFrame,    # âœ… å®˜æ–¹ TTS åœæ­¢å¸§
    TranscriptionFrame,  # âœ… ç”¨äº LLMUserContextAggregator
    UserStartedSpeakingFrame,  # âœ… ç”¨æˆ·å¼€å§‹è¯´è¯
    UserStoppedSpeakingFrame,  # âœ… è§¦å‘ LLM å¤„ç†
    EndFrame,
)


# ==================== Sherpa-ONNX KWS Processor ====================

class SherpaKWSProcessor(FrameProcessor):
    """
    Sherpa-ONNX KWS é€‚é…å™¨ï¼ˆç®€åŒ–ç‰ˆ - é…åˆ Pipecat VADï¼‰

    æ”¹è¿›ï¼š
    - âœ… æ£€æµ‹åˆ°å”¤é†’è¯æ—¶å‘é€ InterruptionFrameï¼ˆä¸­æ–­ TTSï¼‰
    - âœ… ä¾èµ– Pipecat VAD è‡ªåŠ¨æ£€æµ‹ç”¨æˆ·åç»­è¯´è¯
    - âœ… èŒè´£æ˜ç¡®ï¼Œé€»è¾‘ç®€åŒ–
    """

    def __init__(self, kws_model):
        super().__init__()
        self.kws_model = kws_model
        self.kws_stream = kws_model.create_stream()
        self.sample_rate = 16000
        self.is_awake = False  # ç”¨äºå¹¶è¡Œ Pipeline çš„æ¡ä»¶åˆ¤æ–­
        self.last_keyword = None  # ä¿å­˜æœ€åæ£€æµ‹åˆ°çš„å”¤é†’è¯

    async def process_frame(self, frame, direction):
        """å¤„ç†éŸ³é¢‘å¸§ï¼Œæ£€æµ‹å”¤é†’è¯"""
        await super().process_frame(frame, direction)

        if isinstance(frame, AudioRawFrame):
            # æå–éŸ³é¢‘æ•°æ®
            audio_data = np.frombuffer(frame.audio, dtype=np.int16).astype(np.float32) / 32768.0

            # å–‚å…¥ KWS æ¨¡å‹
            self.kws_stream.accept_waveform(self.sample_rate, audio_data)

            # æ£€æµ‹å…³é”®è¯
            while self.kws_model.is_ready(self.kws_stream):
                self.kws_model.decode_stream(self.kws_stream)

            result = self.kws_model.get_result(self.kws_stream)

            if result:
                print(f"ğŸ”” æ£€æµ‹åˆ°å”¤é†’è¯: {result}")
                self.is_awake = True
                self.last_keyword = result

                # âœ… åªå‘é€ä¸­æ–­å¸§ï¼ˆä¸­æ–­ TTSï¼Œå¦‚æœæ­£åœ¨æ’­æ”¾ï¼‰
                # æ³¨æ„ï¼šä¸å‘é€ UserStartedSpeakingFrameï¼Œè®© VAD è‡ªåŠ¨æ£€æµ‹ç”¨æˆ·åç»­è¯´è¯
                await self.push_frame(
                    InterruptionFrame(),
                    direction
                )

                # é‡ç½® KWS æµ
                self.kws_stream = self.kws_model.create_stream()

            # ç»§ç»­ä¼ é€’éŸ³é¢‘å¸§ï¼ˆä¾›åç»­å¤„ç†å™¨ä½¿ç”¨ï¼‰
            await self.push_frame(frame, direction)
        else:
            # å…¶ä»–å¸§ç›´æ¥ä¼ é€’
            await self.push_frame(frame, direction)


# ==================== Sherpa-ONNX ASR Processor ====================

class SherpaASRProcessor(FrameProcessor):
    """
    Sherpa-ONNX ASR é€‚é…å™¨ï¼ˆä¸´æ—¶ç‰ˆæœ¬ - å†…ç½®ç®€å• VADï¼‰

    æ³¨æ„ï¼š
    - âš ï¸ å½“å‰ä½¿ç”¨ç®€å•çš„ RMS VADï¼ˆå› ä¸º Pipecat VAD æš‚æ—¶ç¦ç”¨ï¼‰
    - âœ… å“åº” InterruptionFrame å¼€å§‹å½•éŸ³ï¼ˆæ¥è‡ª KWSï¼‰
    - âœ… ä½¿ç”¨éŸ³é‡æ£€æµ‹åˆ¤æ–­åœæ­¢è¯´è¯
    - âœ… æ·»åŠ è¶…æ—¶ä¿æŠ¤

    TODO: é›†æˆ Pipecat SileroVADAnalyzer åç§»é™¤æ­¤é€»è¾‘
    """

    def __init__(self, asr_model, sample_rate=16000):
        super().__init__()
        self.asr_model = asr_model
        self.sample_rate = sample_rate

        # å½•éŸ³çŠ¶æ€
        self.recording = False
        self.buffer = []
        self.frame_count = 0

        # âš ï¸ ä¸´æ—¶ä½¿ç”¨ç®€å• VADï¼ˆå¾…æ›¿æ¢ä¸º Pipecat VADï¼‰
        self.silence_threshold = 0.02  # RMS é˜ˆå€¼
        self.max_silence_frames = 20   # çº¦ 0.64 ç§’é™éŸ³
        self.silence_count = 0
        self.has_speech = False

        # âœ… è¶…æ—¶ä¿æŠ¤ï¼ˆé˜²æ­¢æ— é™å½•éŸ³ï¼‰
        self.max_record_frames = 300  # çº¦ 10 ç§’ï¼ˆ300å¸§ Ã— 32msï¼‰
        self.max_record_duration = 10.0  # ç§’

    async def process_frame(self, frame, direction):
        """å¤„ç†éŸ³é¢‘å¸§ï¼Œè¯†åˆ«è¯­éŸ³"""
        await super().process_frame(frame, direction)

        # âœ… å“åº” KWS çš„ä¸­æ–­ä¿¡å·ï¼ˆå¼€å§‹å½•éŸ³ï¼‰
        if isinstance(frame, InterruptionFrame):
            print("ğŸ“ æ£€æµ‹åˆ°å”¤é†’è¯ï¼Œå¼€å§‹å½•éŸ³...")
            self.recording = True
            self.buffer = []
            self.frame_count = 0
            self.silence_count = 0
            self.has_speech = False
            await self.push_frame(frame, direction)
            return

        # âœ… å½•éŸ³è¿‡ç¨‹ï¼ˆä½¿ç”¨ç®€å• RMS VADï¼‰
        if self.recording and isinstance(frame, AudioRawFrame):
            # æå–éŸ³é¢‘æ•°æ®
            audio_data = np.frombuffer(frame.audio, dtype=np.int16).astype(np.float32) / 32768.0
            self.buffer.append(audio_data)
            self.frame_count += 1

            # âš ï¸ ç®€å• VADï¼šè®¡ç®—éŸ³é‡
            volume = np.sqrt(np.mean(audio_data**2))

            if volume >= self.silence_threshold:
                self.has_speech = True
                self.silence_count = 0
            else:
                if self.has_speech:  # åªæœ‰åœ¨æ£€æµ‹åˆ°è¯­éŸ³åæ‰è®¡ç®—é™éŸ³
                    self.silence_count += 1

            # æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢å½•éŸ³ï¼ˆé™éŸ³åˆ¤æ–­ï¼‰
            if self.has_speech and self.silence_count > self.max_silence_frames:
                print(f"âœ“ æ£€æµ‹åˆ°é™éŸ³ï¼ˆ{self.silence_count} å¸§ï¼‰ï¼Œå¼€å§‹è¯†åˆ«...")

                # æ‹¼æ¥éŸ³é¢‘
                full_audio = np.concatenate(self.buffer)

                # ASR è¯†åˆ«
                text = await self._recognize_async(full_audio)

                if text:
                    print(f"âœ“ è¯†åˆ«ç»“æœ: {text}")
                    # âœ… ä½¿ç”¨ TranscriptionFrameï¼ˆLLMUserContextAggregator æœŸæœ›çš„ç±»å‹ï¼‰
                    await self.push_frame(
                        TranscriptionFrame(text=text, user_id="user", timestamp=self._get_timestamp()),
                        direction
                    )

                # é‡ç½®çŠ¶æ€
                self.recording = False
                self.buffer = []
                self.frame_count = 0
                self.silence_count = 0
                self.has_speech = False
                return

            # âœ… è¶…æ—¶ä¿æŠ¤
            if self.frame_count > self.max_record_frames:
                print(f"âš ï¸ å½•éŸ³è¶…æ—¶ï¼ˆ{self.max_record_duration}ç§’ï¼‰ï¼Œå¼ºåˆ¶åœæ­¢")
                # å¼ºåˆ¶è§¦å‘è¯†åˆ«
                if self.buffer:
                    full_audio = np.concatenate(self.buffer)
                    text = await self._recognize_async(full_audio)
                    if text:
                        print(f"âœ“ è¯†åˆ«ç»“æœï¼ˆè¶…æ—¶ï¼‰: {text}")
                        await self.push_frame(
                            TranscriptionFrame(text=text, user_id="user", timestamp=self._get_timestamp()),
                            direction
                        )

                # é‡ç½®çŠ¶æ€
                self.recording = False
                self.buffer = []
                self.frame_count = 0
                self.silence_count = 0
                self.has_speech = False
                return

            # ç»§ç»­ä¼ é€’éŸ³é¢‘å¸§
            await self.push_frame(frame, direction)
        else:
            # å…¶ä»–å¸§ç›´æ¥ä¼ é€’
            await self.push_frame(frame, direction)

    async def _recognize_async(self, audio_data):
        """å¼‚æ­¥ ASR è¯†åˆ«ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰"""
        def _recognize_sync():
            # åˆ›å»º ASR æµ
            asr_stream = self.asr_model.create_stream()
            asr_stream.accept_waveform(self.sample_rate, audio_data)
            self.asr_model.decode_stream(asr_stream)
            return asr_stream.result.text.strip()

        # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼ˆé¿å…é˜»å¡äº‹ä»¶å¾ªç¯ï¼‰
        return await asyncio.to_thread(_recognize_sync)

    def _get_timestamp(self):
        """è·å–å½“å‰æ—¶é—´æˆ³ï¼ˆISO 8601 æ ¼å¼ï¼‰"""
        from datetime import datetime, timezone
        return datetime.now(timezone.utc).isoformat()


# ==================== React Agent Processor ====================

class ReactAgentProcessor(FrameProcessor):
    """
    React Agent Processor - åŸºäº MCP å®˜æ–¹æ¨èæ¨¡å¼

    ä½¿ç”¨å®Œå…¨å¼‚æ­¥çš„å®ç°ï¼š
    - ç›´æ¥è°ƒç”¨ agent.execute_command_async()
    - åœ¨ä¸»äº‹ä»¶å¾ªç¯ä¸­æ‰§è¡Œ MCP è°ƒç”¨
    - åå°ä»»åŠ¡æ‰§è¡Œï¼Œä¸é˜»å¡ Pipeline
    - å“åº”å®˜æ–¹ InterruptionFrame ä¸­æ–­ä¿¡å·
    """

    def __init__(self, react_agent):
        """
        åˆå§‹åŒ– React Agent Processor

        Args:
            react_agent: ReactAgent å®ä¾‹ï¼ˆå·²åˆå§‹åŒ– MCPï¼‰
        """
        super().__init__()
        self.agent = react_agent
        self.current_task = None
        self.cancel_flag = False

    async def process_frame(self, frame, direction):
        """å¤„ç†å¸§"""
        await super().process_frame(frame, direction)

        # âœ… å“åº”å®˜æ–¹ä¸­æ–­å¸§ï¼Œå–æ¶ˆå½“å‰ä»»åŠ¡
        if isinstance(frame, InterruptionFrame):
            if self.current_task and not self.current_task.done():
                print("â¸ï¸  æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œå–æ¶ˆå½“å‰ Agent ä»»åŠ¡")
                self.cancel_flag = True
                self.current_task.cancel()
            await self.push_frame(frame, direction)
            return

        # å¤„ç†æ–‡æœ¬å‘½ä»¤
        if isinstance(frame, TextFrame):
            print(f"ğŸ¤– React Agent å¤„ç†: {frame.text}")
            self.cancel_flag = False

            # åˆ›å»ºåå°ä»»åŠ¡ï¼ˆä¸ç­‰å¾…å®Œæˆï¼‰
            self.current_task = asyncio.create_task(
                self._execute_and_push_result(frame.text, direction)
            )

            # ç«‹å³ä¼ é€’å¸§ï¼Œä¸é˜»å¡ Pipeline
            await self.push_frame(frame, direction)
        else:
            # å…¶ä»–å¸§ç›´æ¥ä¼ é€’
            await self.push_frame(frame, direction)

    async def _execute_and_push_result(self, command: str, direction):
        """
        æ‰§è¡Œå‘½ä»¤å¹¶æ¨é€ç»“æœï¼ˆåå°ä»»åŠ¡ï¼‰

        åŸºäº MCP å®˜æ–¹æ¨èæ¨¡å¼ï¼šç›´æ¥å¼‚æ­¥è°ƒç”¨ï¼Œæ— éœ€çº¿ç¨‹
        """
        try:
            print(f"âœ¨ å¼€å§‹æ‰§è¡Œå‘½ä»¤: {command}")

            # ä½¿ç”¨å®˜æ–¹æ¨èçš„å¼‚æ­¥æ–¹å¼ç›´æ¥è°ƒç”¨
            result = await self.agent.execute_command_async(command, enable_voice=False)

            print(f"âœ… æ‰§è¡Œå®Œæˆ: success={result.get('success')}")

            # æ£€æŸ¥æ˜¯å¦è¢«å–æ¶ˆ
            if self.cancel_flag:
                print("â¸ï¸  ä»»åŠ¡å·²å–æ¶ˆï¼Œä¸æ¨é€ç»“æœ")
                return

            # æ¨é€ç»“æœæ–‡æœ¬ï¼ˆå¦‚æœæˆåŠŸï¼‰
            if result.get("success") and result.get("message"):
                result_frame = TextFrame(result["message"])
                await self.push_frame(result_frame, direction)

        except asyncio.CancelledError:
            print("â¸ï¸  ä»»åŠ¡è¢«å–æ¶ˆ")
            raise
        except Exception as e:
            print(f"âŒ æ‰§è¡Œå¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()



# ==================== Piper TTS Processor ====================

class PiperTTSProcessor(FrameProcessor):
    """
    Piper TTS é€‚é…å™¨ - å¥å­çº§æµå¼æ’­æ”¾ï¼ˆç¬¦åˆ Pipecat æ ‡å‡†ï¼‰

    æ”¹è¿›ï¼š
    - âœ… ç”Ÿæˆæ ‡å‡† OutputAudioRawFrameï¼ˆä¸ç›´æ¥æ’­æ”¾ï¼‰
    - âœ… è®© transport.output() è´Ÿè´£å®é™…æ’­æ”¾
    - âœ… æ”¯æŒä¸­æ–­å’Œå¥å­çº§ç¼“å†²
    """

    def __init__(self, tts_manager):
        super().__init__()
        self.tts = tts_manager
        self.interrupt_flag = False  # ä¸­æ–­æ ‡å¿—
        self.is_speaking = False  # TTS æ’­æ”¾çŠ¶æ€

        # å¥å­ç¼“å†²åŒºï¼ˆæŒ‰å¥å­æµå¼æ’­æ”¾ï¼‰
        self.sentence_buffer = ""
        self.sentence_delimiters = ["ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", "\n"]

    def interrupt(self):
        """ä¸­æ–­å½“å‰TTSæ’­æ”¾"""
        self.interrupt_flag = True

    async def process_frame(self, frame, direction):
        """å¤„ç†æ–‡æœ¬å¸§ï¼Œç”Ÿæˆ TTS éŸ³é¢‘"""
        await super().process_frame(frame, direction)

        # å“åº”å®˜æ–¹ä¸­æ–­å¸§ï¼Œè®¾ç½®ä¸­æ–­
        if isinstance(frame, InterruptionFrame):
            if self.is_speaking:
                print("â¸ï¸  æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œåœæ­¢TTSæ’­æ”¾")
                self.interrupt()
            # æ¸…ç©ºç¼“å†²åŒº
            self.sentence_buffer = ""
            # ä¼ é€’ä¸­æ–­å¸§
            await self.push_frame(frame, direction)
            return

        # æ£€æµ‹ LLM å“åº”ç»“æŸï¼ˆæ’­æ”¾å‰©ä½™ç¼“å†²ï¼‰
        from pipecat.frames.frames import LLMFullResponseEndFrame
        if isinstance(frame, LLMFullResponseEndFrame):
            # æ’­æ”¾å‰©ä½™çš„ä¸å®Œæ•´å¥å­
            if self.sentence_buffer.strip():
                print(f"ğŸ”Š TTS åˆæˆï¼ˆå‰©ä½™ï¼‰: {self.sentence_buffer}")
                await self._synthesize_and_push(self.sentence_buffer)
                self.sentence_buffer = ""

            # å‘é€ turn ç»“æŸä¿¡å·
            if not self.is_speaking:
                await self.push_frame(UserStoppedSpeakingFrame(), direction)

            await self.push_frame(frame, direction)
            return

        # æµå¼å¤„ç†æ–‡æœ¬å¸§ï¼ˆå¥å­çº§ç¼“å†²ï¼‰
        if isinstance(frame, TextFrame):
            self.sentence_buffer += frame.text

            # æ£€æŸ¥æ˜¯å¦æœ‰å®Œæ•´å¥å­
            for delimiter in self.sentence_delimiters:
                if delimiter in self.sentence_buffer:
                    # åˆ†å‰²å¥å­
                    parts = self.sentence_buffer.split(delimiter, 1)
                    sentence = parts[0] + delimiter  # åŒ…å«æ ‡ç‚¹ç¬¦å·
                    self.sentence_buffer = parts[1] if len(parts) > 1 else ""

                    # ç«‹å³åˆæˆå®Œæ•´å¥å­
                    print(f"ğŸ”Š TTS åˆæˆ: {sentence.strip()}")
                    await self._synthesize_and_push(sentence.strip())
                    break

            # ä¼ é€’åŸå§‹æ–‡æœ¬å¸§
            await self.push_frame(frame, direction)
        else:
            # å…¶ä»–å¸§ç›´æ¥ä¼ é€’
            await self.push_frame(frame, direction)

    async def _synthesize_and_push(self, text: str):
        """
        å¼‚æ­¥åˆæˆå¹¶æ¨é€éŸ³é¢‘å¸§ï¼ˆç¬¦åˆ Pipecat æ ‡å‡†ï¼‰

        æ”¹è¿›ï¼šç”Ÿæˆ OutputAudioRawFrame è€Œä¸æ˜¯ç›´æ¥æ’­æ”¾
        """
        if not text:
            return

        self.interrupt_flag = False
        self.is_speaking = True

        # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œåˆæˆ
        was_interrupted = await asyncio.to_thread(
            self._synthesize_and_push_sync,
            text
        )

        self.is_speaking = False
        if was_interrupted:
            print("â¸ï¸  TTS å·²è¢«ä¸­æ–­")
            await self.push_frame(TTSStoppedFrame(), FrameDirection.DOWNSTREAM)

    def _synthesize_and_push_sync(self, text: str) -> bool:
        """
        åŒæ­¥ TTS åˆæˆï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰ï¼Œæ”¯æŒä¸­æ–­

        æ”¹è¿›ï¼šç”Ÿæˆ OutputAudioRawFrame è€Œä¸æ˜¯ç›´æ¥æ’­æ”¾

        Returns:
            bool: æ˜¯å¦è¢«ä¸­æ–­
        """
        if self.tts.engine_type == "piper":
            try:
                # ä½¿ç”¨ Piper TTS ç”ŸæˆéŸ³é¢‘
                audio_generator = self.tts.piper_voice.synthesize(text)

                for chunk in audio_generator:
                    # æ£€æŸ¥ä¸­æ–­æ ‡å¿—
                    if self.interrupt_flag:
                        print("â¸ï¸  TTS åˆæˆå·²ä¸­æ–­")
                        return True  # è¿”å›ä¸­æ–­æ ‡å¿—

                    # æå–éŸ³é¢‘æ•°æ®
                    audio_float = chunk.audio_float_array
                    sample_rate = chunk.sample_rate

                    # è½¬æ¢ä¸º int16
                    audio_int16 = (audio_float * 32767).astype(np.int16)

                    # âœ… ç”Ÿæˆæ ‡å‡† OutputAudioRawFrameï¼ˆè€Œä¸æ˜¯ç›´æ¥æ’­æ”¾ï¼‰
                    audio_frame = OutputAudioRawFrame(
                        audio=audio_int16.tobytes(),
                        sample_rate=sample_rate,
                        num_channels=1
                    )

                    # âœ… æ¨é€åˆ° Pipelineï¼ˆè®© transport.output() æ’­æ”¾ï¼‰
                    # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä½¿ç”¨åŒæ­¥æ–¹å¼æ¨é€ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­ï¼‰
                    # ä½¿ç”¨ asyncio.run_coroutine_threadsafe åœ¨ä¸»äº‹ä»¶å¾ªç¯ä¸­æ¨é€
                    import asyncio
                    loop = asyncio.get_event_loop()
                    future = asyncio.run_coroutine_threadsafe(
                        self.push_frame(audio_frame, FrameDirection.DOWNSTREAM),
                        loop
                    )
                    # ç­‰å¾…æ¨é€å®Œæˆ
                    future.result(timeout=1.0)

                return False  # æ­£å¸¸å®Œæˆï¼Œæœªä¸­æ–­

            except Exception as e:
                print(f"âŒ TTS ç”Ÿæˆå¤±è´¥: {e}")
                return False

        return False


# ==================== Vision Processor (ç¬¦åˆ Pipecat æ ‡å‡†) ====================

class VisionProcessor(FrameProcessor):
    """
    Vision Processor - ç¬¦åˆ Pipecat å®˜æ–¹æ¨èæ¨¡å¼

    æ¶æ„æ”¹è¿›ï¼š
    - âœ… æ¥æ”¶ LLMContextï¼Œç›´æ¥ä¿®æ”¹ contextï¼ˆè€Œä¸æ˜¯æ¨é€æ–° Frameï¼‰
    - âœ… åœ¨ user_aggregator ä¹‹åè¿è¡Œï¼ˆcontext å·²åŒ…å«ç”¨æˆ·æ¶ˆæ¯ï¼‰
    - âœ… Vision ç»“æœæ·»åŠ åˆ° contextï¼ŒLLM è‡ªåŠ¨çœ‹åˆ°
    - âœ… æ— éœ€æ¨é€é¢å¤– Frameï¼Œç¬¦åˆå®˜æ–¹æ¶æ„
    """

    def __init__(self, api_url: str, api_key: str, context):
        super().__init__()
        self.api_url = api_url
        self.api_key = api_key
        self.context = context  # LLMContext å®ä¾‹

        # Vision å…³é”®è¯
        self.vision_keywords = [
            "çœ‹", "æŸ¥çœ‹", "è®²è§£", "æè¿°", "æ˜¾ç¤ºä»€ä¹ˆ", "æ˜¾ç¤ºçš„",
            "åˆ†æ", "è¯†åˆ«", "å†…å®¹æ˜¯", "ç”»é¢", "æˆªå›¾", "å›¾ç‰‡",
            "ç•Œé¢", "å½“å‰", "å±å¹•"
        ]
        self.operation_keywords = [
            "ç‚¹å‡»", "è¾“å…¥", "æ‰“å¼€", "å…³é—­", "å¯åŠ¨", "åˆ‡æ¢",
            "æ»šåŠ¨", "æœç´¢", "æ‰§è¡Œ", "è¿è¡Œ", "æŒ‰"
        ]

    def _needs_vision(self, text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦è§†è§‰ç†è§£"""
        # æ“ä½œå…³é”®è¯ä¼˜å…ˆï¼ˆReact æ¨¡å¼ï¼‰
        if any(kw in text for kw in self.operation_keywords):
            return False
        # è§†è§‰å…³é”®è¯
        return any(kw in text for kw in self.vision_keywords)

    async def _capture_screenshot_async(self) -> tuple[bytes, tuple[int, int]]:
        """å¼‚æ­¥æˆªå›¾ï¼Œè¿”å› (å›¾ç‰‡å­—èŠ‚, å°ºå¯¸)"""
        def capture():
            from PIL import ImageGrab
            import io

            # å°è¯•çª—å£æˆªå›¾
            try:
                import ctypes
                from ctypes import wintypes

                # è®¾ç½® DPI æ„ŸçŸ¥
                try:
                    ctypes.windll.shcore.SetProcessDpiAwareness(2)
                except:
                    pass

                # è·å–å‰å°çª—å£
                hwnd = ctypes.windll.user32.GetForegroundWindow()
                if hwnd:
                    rect = wintypes.RECT()
                    ctypes.windll.user32.GetWindowRect(hwnd, ctypes.byref(rect))

                    # ä¿®æ­£è¾¹æ¡†
                    padding = 8
                    bbox = (
                        rect.left + padding,
                        rect.top,
                        rect.right - padding,
                        rect.bottom - padding
                    )
                    screenshot = ImageGrab.grab(bbox=bbox)
                else:
                    screenshot = ImageGrab.grab()
            except Exception:
                # é™çº§åˆ°å…¨å±
                screenshot = ImageGrab.grab()

            # è½¬æ¢ä¸ºå­—èŠ‚ï¼ˆå†…å­˜æ“ä½œï¼‰
            img_byte_arr = io.BytesIO()
            screenshot.save(img_byte_arr, format='PNG')
            return img_byte_arr.getvalue(), screenshot.size

        return await asyncio.to_thread(capture)

    async def _call_vision_api(self, img_bytes: bytes, question: str) -> str:
        """è°ƒç”¨ Qwen-VL-Max APIï¼ˆå¼‚æ­¥ï¼‰"""
        import httpx
        import base64

        img_base64 = base64.b64encode(img_bytes).decode()

        try:
            async with httpx.AsyncClient(timeout=60) as client:
                response = await client.post(
                    f"{self.api_url}/chat/completions",
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": "qwen-vl-max",
                        "messages": [{
                            "role": "user",
                            "content": [
                                {"type": "text", "text": question},
                                {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{img_base64}"}}
                            ]
                        }],
                        "max_tokens": 2000,
                        "temperature": 0.7
                    }
                )

            if response.status_code == 200:
                result = response.json()
                return result["choices"][0]["message"]["content"]
            else:
                return f"APIé”™è¯¯ {response.status_code}: {response.text}"

        except Exception as e:
            return f"Vision API è°ƒç”¨å¤±è´¥: {str(e)}"

    async def process_frame(self, frame, direction):
        """å¤„ç†å¸§"""
        await super().process_frame(frame, direction)

        # å“åº”ä¸­æ–­
        if isinstance(frame, InterruptionFrame):
            await self.push_frame(frame, direction)
            return

        # âœ… æ£€æŸ¥ context ä¸­çš„æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ï¼ˆç”± user_aggregator æ·»åŠ ï¼‰
        if self.context.messages:
            last_message = self.context.messages[-1]

            # åªå¤„ç†ç”¨æˆ·æ¶ˆæ¯
            if last_message.get("role") == "user":
                text_content = last_message.get("content", "")

                if self._needs_vision(text_content):
                    print(f"ğŸ” Vision æ¨¡å¼: {text_content}")

                    try:
                        # å¼‚æ­¥æˆªå›¾
                        img_bytes, size = await self._capture_screenshot_async()

                        # è°ƒç”¨ Vision API
                        print(f"ğŸ“¸ è°ƒç”¨ Vision API...")
                        result = await self._call_vision_api(img_bytes, text_content)

                        print(f"ğŸ“Š Vision ç»“æœ: {result}")

                        # âœ… ç›´æ¥ä¿®æ”¹ contextï¼ˆæ·»åŠ  Vision è§‚å¯Ÿç»“æœï¼‰
                        # æ–¹å¼1ï¼šä½œä¸º system æ¶ˆæ¯
                        self.context.messages.append({
                            "role": "system",
                            "content": f"[è§†è§‰è§‚å¯Ÿ] {result}"
                        })

                        # æ–¹å¼2ï¼šä¿®æ”¹ç”¨æˆ·æ¶ˆæ¯ï¼ˆåŒ…å« Vision ç»“æœï¼‰
                        # last_message["content"] = f"{text_content}\n\n[è§†è§‰è§‚å¯Ÿ] {result}"

                    except Exception as e:
                        print(f"âŒ Vision å¤„ç†å¤±è´¥: {e}")
                        import traceback
                        traceback.print_exc()

        # âœ… ä¼ é€’æ‰€æœ‰å¸§ï¼ˆä¸æ¨é€æ–° Frameï¼‰
        await self.push_frame(frame, direction)
