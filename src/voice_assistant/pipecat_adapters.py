"""Pipecat é€‚é…å™¨ - å°è£…ç°æœ‰ç»„ä»¶ä¸º Pipecat Processors"""
import asyncio
import numpy as np
import tempfile
import ctypes
from ctypes import wintypes
from pathlib import Path
from typing import Optional
from dataclasses import dataclass
from PIL import ImageGrab

from pipecat.processors.frame_processor import FrameProcessor
from pipecat.frames.frames import (
    Frame,
    AudioRawFrame,
    TextFrame,
    TTSAudioRawFrame,
    InterruptionFrame,  # âœ… å®˜æ–¹ä¸­æ–­å¸§
    TTSStoppedFrame,    # âœ… å®˜æ–¹ TTS åœæ­¢å¸§
    EndFrame,
)


# ==================== Sherpa-ONNX KWS Processor ====================

class SherpaKWSProcessor(FrameProcessor):
    """
    Sherpa-ONNX KWS é€‚é…å™¨

    å°†ç°æœ‰çš„ Sherpa-ONNX KWS æ¨¡å‹å°è£…ä¸º Pipecat Processor
    å¤„ç†éŸ³é¢‘å¸§ï¼Œæ£€æµ‹å”¤é†’è¯ï¼Œè¾“å‡ºå®˜æ–¹ InterruptionFrame
    """

    def __init__(self, kws_model):
        super().__init__()
        self.kws_model = kws_model
        self.kws_stream = kws_model.create_stream()
        self.sample_rate = 16000
        self.is_awake = False  # ç”¨äºå¹¶è¡Œ Pipeline çš„æ¡ä»¶åˆ¤æ–­
        self.last_keyword = None  # ä¿å­˜æœ€åæ£€æµ‹åˆ°çš„å”¤é†’è¯

    async def process_frame(self, frame, direction):
        """å¤„ç†éŸ³é¢‘å¸§ï¼Œæ£€æµ‹å”¤é†’è¯"""
        await super().process_frame(frame, direction)

        if isinstance(frame, AudioRawFrame):
            # æå–éŸ³é¢‘æ•°æ®
            audio_data = np.frombuffer(frame.audio, dtype=np.int16).astype(np.float32) / 32768.0

            # å–‚å…¥ KWS æ¨¡å‹
            self.kws_stream.accept_waveform(self.sample_rate, audio_data)

            # æ£€æµ‹å…³é”®è¯
            while self.kws_model.is_ready(self.kws_stream):
                self.kws_model.decode_stream(self.kws_stream)

            result = self.kws_model.get_result(self.kws_stream)

            if result:
                print(f"ğŸ”” æ£€æµ‹åˆ°å”¤é†’è¯: {result}")
                self.is_awake = True
                self.last_keyword = result

                # âœ… å‘å‡ºå®˜æ–¹ä¸­æ–­å¸§ï¼ˆPipecat æ ‡å‡†ï¼‰
                await self.push_frame(
                    InterruptionFrame(),
                    direction
                )

                # é‡ç½® KWS æµ
                self.kws_stream = self.kws_model.create_stream()

            # ç»§ç»­ä¼ é€’éŸ³é¢‘å¸§ï¼ˆä¾›åç»­å¤„ç†å™¨ä½¿ç”¨ï¼‰
            await self.push_frame(frame, direction)
        else:
            # å…¶ä»–å¸§ç›´æ¥ä¼ é€’
            await self.push_frame(frame, direction)


# ==================== Sherpa-ONNX ASR Processor ====================

class SherpaASRProcessor(FrameProcessor):
    """
    Sherpa-ONNX ASR é€‚é…å™¨

    å°†ç°æœ‰çš„ Sherpa-ONNX ASR æ¨¡å‹å°è£…ä¸º Pipecat Processor
    æ£€æµ‹åˆ°å”¤é†’è¯åå¼€å§‹å½•éŸ³ï¼Œä½¿ç”¨é™éŸ³æ£€æµ‹è‡ªåŠ¨åœæ­¢ï¼Œè¾“å‡ºè¯†åˆ«æ–‡æœ¬
    """

    def __init__(self, asr_model, sample_rate=16000):
        super().__init__()
        self.asr_model = asr_model
        self.sample_rate = sample_rate

        # å½•éŸ³çŠ¶æ€
        self.recording = False
        self.buffer = []

        # é™éŸ³æ£€æµ‹å‚æ•°ï¼ˆä¸åŸæœ‰é€»è¾‘ä¸€è‡´ï¼‰
        self.silence_threshold = 0.02
        self.max_silence_frames = 20  # çº¦ 1.3 ç§’
        self.min_record_frames = 15   # æœ€å°å½•éŸ³ä¿æŠ¤

        self.silence_count = 0
        self.has_speech = False
        self.frame_count = 0

    async def process_frame(self, frame, direction):
        """å¤„ç†éŸ³é¢‘å¸§ï¼Œè¯†åˆ«è¯­éŸ³"""
        await super().process_frame(frame, direction)

        # âœ… æ£€æµ‹å®˜æ–¹ä¸­æ–­å¸§ï¼Œå¼€å§‹å½•éŸ³
        if isinstance(frame, InterruptionFrame):
            print("ğŸ“ å¼€å§‹å½•éŸ³è¯†åˆ«...")
            self.recording = True
            self.buffer = []
            self.silence_count = 0
            self.has_speech = False
            self.frame_count = 0

            # ä¼ é€’ä¸­æ–­å¸§
            await self.push_frame(frame, direction)
            return

        # å½•éŸ³è¿‡ç¨‹
        if self.recording and isinstance(frame, AudioRawFrame):
            # æå–éŸ³é¢‘æ•°æ®
            audio_data = np.frombuffer(frame.audio, dtype=np.int16).astype(np.float32) / 32768.0
            self.buffer.append(audio_data)
            self.frame_count += 1

            # è®¡ç®—éŸ³é‡
            volume = np.sqrt(np.mean(audio_data**2))

            # é™éŸ³æ£€æµ‹
            if volume >= self.silence_threshold:
                self.has_speech = True
                self.silence_count = 0
            else:
                self.silence_count += 1

            # åœæ­¢æ¡ä»¶ï¼šæœ‰è¯­éŸ³ + è¿ç»­é™éŸ³ + è¶…è¿‡æœ€å°ä¿æŠ¤å¸§
            if (self.has_speech and
                self.silence_count > self.max_silence_frames and
                self.frame_count > self.min_record_frames):

                # æ‹¼æ¥éŸ³é¢‘
                full_audio = np.concatenate(self.buffer)

                # ASR è¯†åˆ«
                text = await self._recognize_async(full_audio)

                if text:
                    print(f"âœ“ è¯†åˆ«ç»“æœ: {text}")
                    await self.push_frame(
                        TextFrame(text=text),
                        direction
                    )

                # é‡ç½®çŠ¶æ€
                self.recording = False
                self.buffer = []

            # ç»§ç»­ä¼ é€’éŸ³é¢‘å¸§
            await self.push_frame(frame, direction)
        else:
            # å…¶ä»–å¸§ç›´æ¥ä¼ é€’
            await self.push_frame(frame, direction)

    async def _recognize_async(self, audio_data):
        """å¼‚æ­¥ ASR è¯†åˆ«ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰"""
        def _recognize_sync():
            # åˆ›å»º ASR æµ
            asr_stream = self.asr_model.create_stream()
            asr_stream.accept_waveform(self.sample_rate, audio_data)
            self.asr_model.decode_stream(asr_stream)
            return asr_stream.result.text.strip()

        # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼ˆé¿å…é˜»å¡äº‹ä»¶å¾ªç¯ï¼‰
        return await asyncio.to_thread(_recognize_sync)


# ==================== React Agent Processor ====================

class ReactAgentProcessor(FrameProcessor):
    """
    React Agent Processor - åŸºäº MCP å®˜æ–¹æ¨èæ¨¡å¼

    ä½¿ç”¨å®Œå…¨å¼‚æ­¥çš„å®ç°ï¼š
    - ç›´æ¥è°ƒç”¨ agent.execute_command_async()
    - åœ¨ä¸»äº‹ä»¶å¾ªç¯ä¸­æ‰§è¡Œ MCP è°ƒç”¨
    - åå°ä»»åŠ¡æ‰§è¡Œï¼Œä¸é˜»å¡ Pipeline
    - å“åº”å®˜æ–¹ InterruptionFrame ä¸­æ–­ä¿¡å·
    """

    def __init__(self, react_agent):
        """
        åˆå§‹åŒ– React Agent Processor

        Args:
            react_agent: ReactAgent å®ä¾‹ï¼ˆå·²åˆå§‹åŒ– MCPï¼‰
        """
        super().__init__()
        self.agent = react_agent
        self.current_task = None
        self.cancel_flag = False

    async def process_frame(self, frame, direction):
        """å¤„ç†å¸§"""
        await super().process_frame(frame, direction)

        # âœ… å“åº”å®˜æ–¹ä¸­æ–­å¸§ï¼Œå–æ¶ˆå½“å‰ä»»åŠ¡
        if isinstance(frame, InterruptionFrame):
            if self.current_task and not self.current_task.done():
                print("â¸ï¸  æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œå–æ¶ˆå½“å‰ Agent ä»»åŠ¡")
                self.cancel_flag = True
                self.current_task.cancel()
            await self.push_frame(frame, direction)
            return

        # å¤„ç†æ–‡æœ¬å‘½ä»¤
        if isinstance(frame, TextFrame):
            print(f"ğŸ¤– React Agent å¤„ç†: {frame.text}")
            self.cancel_flag = False

            # åˆ›å»ºåå°ä»»åŠ¡ï¼ˆä¸ç­‰å¾…å®Œæˆï¼‰
            self.current_task = asyncio.create_task(
                self._execute_and_push_result(frame.text, direction)
            )

            # ç«‹å³ä¼ é€’å¸§ï¼Œä¸é˜»å¡ Pipeline
            await self.push_frame(frame, direction)
        else:
            # å…¶ä»–å¸§ç›´æ¥ä¼ é€’
            await self.push_frame(frame, direction)

    async def _execute_and_push_result(self, command: str, direction):
        """
        æ‰§è¡Œå‘½ä»¤å¹¶æ¨é€ç»“æœï¼ˆåå°ä»»åŠ¡ï¼‰

        åŸºäº MCP å®˜æ–¹æ¨èæ¨¡å¼ï¼šç›´æ¥å¼‚æ­¥è°ƒç”¨ï¼Œæ— éœ€çº¿ç¨‹
        """
        try:
            print(f"âœ¨ å¼€å§‹æ‰§è¡Œå‘½ä»¤: {command}")

            # ä½¿ç”¨å®˜æ–¹æ¨èçš„å¼‚æ­¥æ–¹å¼ç›´æ¥è°ƒç”¨
            result = await self.agent.execute_command_async(command, enable_voice=False)

            print(f"âœ… æ‰§è¡Œå®Œæˆ: success={result.get('success')}")

            # æ£€æŸ¥æ˜¯å¦è¢«å–æ¶ˆ
            if self.cancel_flag:
                print("â¸ï¸  ä»»åŠ¡å·²å–æ¶ˆï¼Œä¸æ¨é€ç»“æœ")
                return

            # æ¨é€ç»“æœæ–‡æœ¬ï¼ˆå¦‚æœæˆåŠŸï¼‰
            if result.get("success") and result.get("message"):
                result_frame = TextFrame(result["message"])
                await self.push_frame(result_frame, direction)

        except asyncio.CancelledError:
            print("â¸ï¸  ä»»åŠ¡è¢«å–æ¶ˆ")
            raise
        except Exception as e:
            print(f"âŒ æ‰§è¡Œå¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()



# ==================== Piper TTS Processor ====================

class PiperTTSProcessor(FrameProcessor):
    """
    Piper TTS é€‚é…å™¨

    å°†ç°æœ‰çš„ TTSManagerStreaming å°è£…ä¸º Pipecat Processor
    æ¥æ”¶æ–‡æœ¬å¸§ï¼Œç”ŸæˆéŸ³é¢‘å¸§å¹¶ç›´æ¥æ’­æ”¾
    å“åº”å®˜æ–¹ InterruptionFrameï¼Œå‘å‡º TTSStoppedFrame
    """

    def __init__(self, tts_manager, transport=None):
        super().__init__()
        self.tts = tts_manager
        self.transport = transport  # ç”¨äºç›´æ¥æ’­æ”¾éŸ³é¢‘
        self.interrupt_flag = False  # ä¸­æ–­æ ‡å¿—
        self.is_speaking = False  # TTS æ’­æ”¾çŠ¶æ€

    def interrupt(self):
        """ä¸­æ–­å½“å‰TTSæ’­æ”¾"""
        self.interrupt_flag = True

    async def process_frame(self, frame, direction):
        """å¤„ç†æ–‡æœ¬å¸§ï¼Œç”Ÿæˆ TTS éŸ³é¢‘"""
        await super().process_frame(frame, direction)

        # âœ… å“åº”å®˜æ–¹ä¸­æ–­å¸§ï¼Œè®¾ç½®ä¸­æ–­
        if isinstance(frame, InterruptionFrame):
            if self.is_speaking:
                print("â¸ï¸  æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œåœæ­¢TTSæ’­æ”¾")
                self.interrupt()
            # ä¼ é€’ä¸­æ–­å¸§
            await self.push_frame(frame, direction)
            return

        if isinstance(frame, TextFrame):
            print(f"ğŸ”Š TTS åˆæˆ: {frame.text}")

            # é‡ç½®ä¸­æ–­æ ‡å¿—ï¼Œæ ‡è®°ä¸ºæ’­æ”¾ä¸­
            self.interrupt_flag = False
            self.is_speaking = True

            # åœ¨çº¿ç¨‹æ± ä¸­ç”Ÿæˆå’Œæ’­æ”¾éŸ³é¢‘
            was_interrupted = await asyncio.to_thread(
                self._synthesize_and_play_sync,
                frame.text
            )

            # âœ… TTS æ’­æ”¾ç»“æŸï¼Œå‘å‡ºå®˜æ–¹åœæ­¢å¸§
            self.is_speaking = False
            if was_interrupted:
                print("â¸ï¸  TTS å·²è¢«ä¸­æ–­")
                await self.push_frame(TTSStoppedFrame(), direction)

            # ä¼ é€’åŸå§‹æ–‡æœ¬å¸§
            await self.push_frame(frame, direction)
        else:
            # å…¶ä»–å¸§ç›´æ¥ä¼ é€’
            await self.push_frame(frame, direction)

    def _synthesize_and_play_sync(self, text) -> bool:
        """
        åŒæ­¥ TTS åˆæˆå¹¶æ’­æ”¾ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰ï¼Œæ”¯æŒä¸­æ–­

        Returns:
            bool: æ˜¯å¦è¢«ä¸­æ–­
        """
        if self.tts.engine_type == "piper":
            try:
                # ä½¿ç”¨ Piper TTS ç”ŸæˆéŸ³é¢‘
                audio_generator = self.tts.piper_voice.synthesize(text)

                for chunk in audio_generator:
                    # æ£€æŸ¥ä¸­æ–­æ ‡å¿—
                    if self.interrupt_flag:
                        print("â¸ï¸  TTS æ’­æ”¾å·²ä¸­æ–­")
                        return True  # è¿”å›ä¸­æ–­æ ‡å¿—

                    # æå–éŸ³é¢‘æ•°æ®
                    audio_float = chunk.audio_float_array
                    sample_rate = chunk.sample_rate

                    # è½¬æ¢ä¸º int16
                    audio_int16 = (audio_float * 32767).astype(np.int16)

                    # ç›´æ¥æ’­æ”¾åˆ° transport
                    if self.transport and self.transport.output_stream:
                        self.transport.output_stream.write(audio_int16.tobytes())

                return False  # æ­£å¸¸å®Œæˆï¼Œæœªä¸­æ–­

            except Exception as e:
                print(f"âŒ TTS ç”Ÿæˆå¤±è´¥: {e}")
                return False


# ==================== Vision Processor ====================

class VisionProcessor(FrameProcessor):
    """
    Vision ç†è§£ Processor

    åˆ¤æ–­ç”¨æˆ·æŒ‡ä»¤æ˜¯å¦éœ€è¦è§†è§‰ç†è§£ï¼Œå¦‚éœ€è¦åˆ™ï¼š
    1. æˆªå›¾ï¼ˆå¼‚æ­¥ï¼‰
    2. è°ƒç”¨ Vision APIï¼ˆå¼‚æ­¥ï¼‰
    3. è¾“å‡ºåˆ†æç»“æœ
    """

    def __init__(self, vision_client):
        super().__init__()
        self.vision = vision_client  # VisionUnderstanding å®ä¾‹
        self.vision_keywords = [
            "çœ‹", "æŸ¥çœ‹", "è®²è§£", "æè¿°", "æ˜¾ç¤ºä»€ä¹ˆ", "æ˜¾ç¤ºçš„",
            "åˆ†æ", "è¯†åˆ«", "å†…å®¹æ˜¯", "ç”»é¢", "æˆªå›¾", "å›¾ç‰‡"
        ]
        self.operation_keywords = [
            "ç‚¹å‡»", "è¾“å…¥", "æ‰“å¼€", "å…³é—­", "å¯åŠ¨", "åˆ‡æ¢",
            "æ»šåŠ¨", "æœç´¢", "æ‰§è¡Œ", "è¿è¡Œ", "æŒ‰"
        ]

    def _needs_vision(self, command: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦ Vision"""
        # æ“ä½œå…³é”®è¯ä¼˜å…ˆï¼ˆReact æ¨¡å¼ï¼‰
        if any(kw in command for kw in self.operation_keywords):
            return False
        # è§†è§‰å…³é”®è¯
        return any(kw in command for kw in self.vision_keywords)

    def _get_foreground_window_rect(self) -> Optional[tuple]:
        """è·å–å‰å°çª—å£åæ ‡ï¼ˆDPIæ„ŸçŸ¥ï¼‰"""
        try:
            # è®¾ç½® DPI æ„ŸçŸ¥
            try:
                ctypes.windll.shcore.SetProcessDpiAwareness(2)
            except:
                pass  # å¯èƒ½å·²ç»è®¾ç½®è¿‡

            # è·å–å‰å°çª—å£å¥æŸ„
            hwnd = ctypes.windll.user32.GetForegroundWindow()
            if not hwnd:
                return None

            # è·å–çª—å£çŸ©å½¢
            rect = wintypes.RECT()
            ctypes.windll.user32.GetWindowRect(hwnd, ctypes.byref(rect))

            # ä¿®æ­£ï¼šå»é™¤è¾¹æ¡†å’Œé˜´å½±
            padding = 8
            bbox = (
                rect.left + padding,
                rect.top,
                rect.right - padding,
                rect.bottom - padding
            )

            return bbox

        except Exception as e:
            print(f"[Vision] è·å–çª—å£åæ ‡å¤±è´¥: {e}")
            return None

    def _take_screenshot_sync(self, target: str = "window") -> str:
        """åŒæ­¥æˆªå›¾é€»è¾‘"""
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        temp_file = tempfile.NamedTemporaryFile(
            suffix='.png',
            delete=False
        )
        temp_path = temp_file.name
        temp_file.close()

        if target == "window":
            # å°è¯•çª—å£æˆªå›¾
            bbox = self._get_foreground_window_rect()
            if bbox:
                screenshot = ImageGrab.grab(bbox=bbox)
            else:
                # é™çº§åˆ°å…¨å±
                print("[Vision] çª—å£æˆªå›¾å¤±è´¥ï¼Œä½¿ç”¨å…¨å±æ¨¡å¼")
                screenshot = ImageGrab.grab()
        else:
            # å…¨å±æˆªå›¾
            screenshot = ImageGrab.grab()

        screenshot.save(temp_path)
        return temp_path

    async def _take_screenshot_async(self) -> str:
        """å¼‚æ­¥æˆªå›¾ï¼ˆä½¿ç”¨çº¿ç¨‹æ± ï¼‰"""
        return await asyncio.to_thread(self._take_screenshot_sync)

    async def process_frame(self, frame, direction):
        """å¤„ç†å¸§"""
        await super().process_frame(frame, direction)

        # å“åº”ä¸­æ–­
        if isinstance(frame, InterruptionFrame):
            await self.push_frame(frame, direction)
            return

        # å¤„ç†æ–‡æœ¬å‘½ä»¤
        if isinstance(frame, TextFrame):
            if self._needs_vision(frame.text):
                print(f"ğŸ” Vision æ¨¡å¼: {frame.text}")

                try:
                    # å¼‚æ­¥æˆªå›¾
                    screenshot_path = await self._take_screenshot_async()

                    # å¼‚æ­¥ Vision API
                    result = await self.vision.understand_screen_async(
                        screenshot_path,
                        question=frame.text
                    )

                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    try:
                        Path(screenshot_path).unlink()
                    except:
                        pass

                    # è¾“å‡ºç»“æœ
                    print(f"ğŸ“Š Vision ç»“æœ: {result[:100]}...")
                    await self.push_frame(TextFrame(result), direction)
                    return

                except Exception as e:
                    print(f"âŒ Vision å¤„ç†å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                    # å¤±è´¥æ—¶ä»ä¼ é€’åŸå§‹å¸§ç»™ Agent
                    await self.push_frame(frame, direction)
                    return

        # é Vision ä»»åŠ¡ï¼Œä¼ é€’ç»™ ReactAgent
        await self.push_frame(frame, direction)
